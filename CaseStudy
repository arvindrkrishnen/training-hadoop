## Check if Hive table already exists through BeesWax
Access browser and type http://192.168.226.130:8080/#/main/hosts where the IP address is for your sandbox. 
Login to BeesWax and execute drop table persons;

### Import into Hive ORC

sqoop import --verbose --connect 'jdbc:mysql://localhost/people' --table persons --username root --hcatalog-table people_person --hcatalog-storage-stanza "stored as orc" -m 1 --create-hcatalog-table 

### Check from Beeswax on how the table has been created.
select * from people_person;

### Notice the table is stored in ORC format by clicking view file location and then on part-m-00000
http://192.168.226.130:8000/filebrowser/view//apps/hive/warehouse/people_person/part-m-00000  

### Import Web History to Web ORC table

Copy file from local to HDFS
hadoop fs -mkdir /user/guest/Webtraffic
hadoop fs -copyFromLocal webtraffic.log /user/guest/Webtraffic

Verify if it got copied
hadoop fs -ls /user/guest/Webtraffic


# Create Table in Beeswax 
create table web_traffic_table(id int, val string) stored as orc row format delimited fields terminated by ',' stored as orc;
load data inpath '/user/guest/Webtraffic/webtraffic.log' into table 'web_traffic_table';

create EXTERNAL table if not exists person_webtraffic (id int, val string) 
partitioned by (year string,month string,day string) 
clustered by (id) into 7 buckets 
stored as orc
LOCATION '/user/hive'
TBLPROPERTIES ("transactional"="true")

#Run Select * query from Beeswax

select * from person_webtraffic;

