## Check if Hive table already exists through BeesWax
Access browser and type http://192.168.226.130:8080/#/main/hosts where the IP address is for your sandbox. 
Login to BeesWax and execute drop table persons;

### Import into Hive ORC

sqoop import --verbose --connect 'jdbc:mysql://localhost/people' --table persons --username root --hcatalog-table people_person --hcatalog-storage-stanza "stored as orc" -m 1 --create-hcatalog-table 

### Check from Beeswax on how the table has been created.
select * from people_person;

### Notice the table is stored in ORC format by clicking view file location and then on part-m-00000
http://192.168.226.130:8000/filebrowser/view//apps/hive/warehouse/people_person/part-m-00000  

### Import Web History to Web ORC table

Copy file from local to HDFS
hadoop fs -mkdir /user/hue/HadoopTraining
hadoop fs -copyFromLocal webtraffic.log /user/hue/HadoopTraining

Verify if it got copied
hadoop fs -ls /user/hue/HadoopTraining

# Create Table in Beeswax UI 

# upload data from your desktop directly to HDFS as well from UI

# Update data onto the table from UI

# you will notice that few records did not get loaded and hence the upload failed. Find out which records did not get uploaded properly?

# Run basic SQL queries in these tables. Verify the performance.

# Install apache maven

curl -o /etc/yum.repos.d/epel-apache-maven.repo https://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo
yum -y install apache-maven


